# CPU Surrogate Trainer — User Instructions

This guide explains how to build, test, benchmark, and extend the CPU Surrogate Trainer library. It assumes you are working at the repository root (the directory that contains this file).

## 1. Prerequisites

1. **Toolchain**
   - CMake ≥ 3.20 (presets rely on built-in preset support).
   - A C++20 compiler with AVX2 and OpenMP support for the fastest builds (GCC 11+, Clang 14+, MSVC 19.29+, or Apple Clang 14+).
   - Ninja or Makefiles; CMake presets default to Ninja when available.
2. **Optional tools**
   - `clang-tidy` to rerun static analysis (diagnostics from the latest run live in `static_analysis_report.txt`).
   - `perf`, `likwid`, or similar profilers if you want to profile kernels.

Before configuring, ensure the compiler is discoverable on your `PATH` (Linux/macOS) or provided via the Visual Studio Developer Command Prompt (Windows).

## 2. Repository Layout

Key directories:

- `include/surrogate/` — public headers for tensors, layers, optimizers, trainer, logging, and persistence APIs.
- `src/` — library implementations linked into the static `surrogate` library.
- `tests/` — unit/regression tests; each file builds into a standalone executable that is registered with CTest.
- `bench/` — GEMM and elementwise benchmark runners.
- `examples/` — small self-contained programs such as `example_mlp` that demonstrate the API.
- `analysis/` — reports generated by the validation pipeline (sanitizer results, benchmark tables, checklist).
- `static_analysis_report.txt` — clang-tidy diagnostics snapshot.

Refer to `analysis/report.md` for the most recent validation summary, including pass/fail status for every build preset and benchmark result.

## 3. Configuring and Building

All supported build recipes are encoded in `CMakePresets.json`.

### 3.1 Configure
Run one of the presets from the repository root:

```bash
cmake --preset Debug-Sanitize
cmake --preset Release-AVX2
cmake --preset Release-Portable
```

The configure step generates build trees under `build/debug-sanitize`, `build/release-avx2`, and `build/release-portable` respectively. The `Release-AVX2` preset automatically enables AVX2 intrinsics and OpenMP when the toolchain supports them (`SUR_ENABLE_AVX2` / `SUR_ENABLE_OPENMP` can be toggled manually via `-D` flags if needed).【F:CMakeLists.txt†L1-L84】【F:CMakePresets.json†L1-L120】

### 3.2 Build
After configuring, build any preset:

```bash
cmake --build --preset Debug-Sanitize
cmake --build --preset Release-AVX2
```

The aggregate static library `surrogate` plus all tests, benchmarks, and examples will be produced inside the corresponding build tree. To build a single target (for example, the MLP example) use `cmake --build --preset Release-AVX2 --target example_mlp`.

## 4. Running Tests

CTest drives each unit test executable that lives under `tests/`. After building, run:

```bash
ctest --preset Debug-Sanitize
```

or

```bash
ctest --preset Release-AVX2 -C Release
```

The sanitizer preset enables AddressSanitizer and UBSan for maximum coverage, while the release preset verifies the optimized AVX2/OpenMP configuration. All tests are also runnable directly (e.g., `./build/debug-sanitize/tests/test_trainer`).【F:CMakeLists.txt†L84-L184】【F:tests/test_trainer.cpp†L1-L104】

## 5. Running the Example MLP

1. Configure and build the desired preset (release recommended for performance).
2. Execute the generated binary:

   ```bash
   ./build/release-avx2/example_mlp
   ```

The example constructs a 3-layer multilayer perceptron, performs a warm-up pass, prints activations, and runs a backward pass to display gradients. Use it as a template for constructing `sur::Model` instances manually.【F:examples/example_mlp.cpp†L1-L52】

## 6. Training Workflow

A typical end-to-end training loop involves tensors, a `TensorDataLoader`, an optimizer, a loss function, and the trainer:

```cpp
sur::Tensor<float> inputs({input_dim, total_samples});
sur::Tensor<float> targets({output_dim, total_samples});
// Populate tensors with your dataset here.

sur::TensorDataLoader loader(std::move(inputs), std::move(targets), batch_size,
                             /*shuffle=*/true, /*seed=*/1337u);

sur::Model model;
model.add(std::make_unique<sur::Dense>(input_dim, hidden_dim));
model.add(std::make_unique<sur::ReLU>());
model.add(std::make_unique<sur::Dense>(hidden_dim, output_dim));
model.reserve_workspaces(batch_size);

sur::SGD optimizer(/*lr=*/0.01f, /*momentum=*/0.9f, /*weight_decay=*/1e-4f);
sur::MSE loss;

sur::TrainConfig config;
config.epochs = 100;
config.batch_size = batch_size;
config.threads = 4;
config.deterministic = true;
config.log_every = 10;
config.seed = 1337u;
config.logger = &sur::default_logger();

sur::Trainer trainer;
trainer.train(model, optimizer, loss, loader, config);
```

Important notes:

- `TensorDataLoader` enforces 32-byte alignment and reshuffles between epochs. Call `set_seed` (or provide a seed in the constructor) for reproducible batches.【F:include/surrogate/dataloader.hpp†L1-L66】【F:src/dataloader.cpp†L1-L200】
- `Model::reserve_workspaces` should be called before training so layer workspaces are sized for the largest batch you expect.【F:include/surrogate/model.hpp†L1-L120】
- Choose between `sur::SGD` and `sur::Adam` depending on your optimizer needs; both expose `set_lr` and `current_lr()` for dynamic schedules.【F:include/surrogate/optimizer.hpp†L1-L120】【F:src/optimizer_adam.cpp†L1-L200】
- Losses (`sur::MSE`, `sur::MAE`) validate tensor shapes and materialize contiguous buffers as needed.【F:include/surrogate/loss.hpp†L1-L80】【F:src/loss.cpp†L1-L200】

## 7. Deterministic Training and Seeding

Enable determinism via `TrainConfig` to force ordered OpenMP reductions, static scheduling, and reproducible dataloader shuffles. Set `config.deterministic = true` and provide a `seed`. For strictly serial behavior, also set `config.threads = 1`. The trainer routes the seed to the active dataloader and configures OpenMP guards during the training loop.【F:src/trainer.cpp†L1-L220】【F:tests/test_trainer.cpp†L47-L103】

## 8. Logging Progress

Use the default CSV logger for lightweight telemetry:

```cpp
sur::configure_default_logger("training_log.csv", /*log_interval=*/5);
config.logger = &sur::default_logger();
```

Per-batch entries record epoch, batch index, loss, learning rate, gradient norm, and elapsed milliseconds. Set `log_every` in `TrainConfig` to control the emission cadence (0 disables logging). Custom loggers can subclass `sur::Logger` and override `on_batch` / `flush`.【F:include/surrogate/logging.hpp†L1-L40】【F:src/logging.cpp†L1-L200】

## 9. Saving and Loading Models

Persist model parameters to disk with the binary serializer:

```cpp
sur::save(model, "model.bin");
// ... later ...
sur::load(model, "model.bin");
```

Metadata guards verify tensor shapes and endianness on load, and parameters are streamed in the same order `model.parameters()` exposes them. Use temporary paths when testing; helper guards in `tests/test_persist.cpp` illustrate safe cleanup patterns.【F:include/surrogate/persist.hpp†L1-L32】【F:src/persist.cpp†L1-L240】【F:tests/test_persist.cpp†L1-L120】

## 10. Running Benchmarks

Build the release preset and execute:

```bash
./build/release-avx2/surrogate_bench_gemm
./build/release-avx2/surrogate_bench_elemwise
```

Each benchmark prints reference vs AVX2 throughput (GFLOP/s) across canonical problem sizes, plus estimated memory bandwidth. Use these numbers to validate CPU-specific tuning or to compare compiler versions.【F:bench/bench_gemm.cpp†L1-L200】【F:bench/bench_elemwise.cpp†L1-L160】

## 11. Static Analysis

To regenerate the clang-tidy report captured in `static_analysis_report.txt`, make sure the `Debug-Sanitize` preset is configured (so compile commands are available) and run:

```bash
for file in src/*.cpp src/**/*.cpp tests/*.cpp bench/*.cpp examples/*.cpp; do
  if [ -f "$file" ]; then
    clang-tidy --quiet -p build/debug-sanitize "$file"
  fi
done
```

Redirect the output to a new report file if you want to compare diagnostics across runs. The existing report preserves tool output from the latest sweep.【F:static_analysis_report.txt†L1-L80】

## 12. Troubleshooting

| Symptom | Remedy |
| --- | --- |
| **`SUR_HAS_AVX2` not defined / AVX2 kernels disabled** | Ensure your compiler supports AVX2 flags; disable via `-DSUR_ENABLE_AVX2=OFF` if targeting older CPUs.【F:CMakeLists.txt†L1-L84】 |
| **OpenMP linker errors** | Install the platform’s OpenMP runtime (e.g., `libomp` on macOS) or rebuild with `-DSUR_ENABLE_OPENMP=OFF`.【F:CMakeLists.txt†L84-L140】 |
| **Determinism drifts between runs** | Use `config.deterministic = true`, fix `config.threads = 1`, and reuse identical seeds. Floating-point associativity may still introduce sub-ulp differences.【F:src/trainer.cpp†L120-L220】 |
| **CSV log empty** | Set `config.log_every` > 0 and ensure `config.logger` references a live logger instance. Flush logs at shutdown via `Logger::flush()`.【F:src/logging.cpp†L1-L200】 |
| **Persistence load failure** | Confirm the model topology matches the saved file; the loader verifies layer tags and tensor shapes before restoring parameters.【F:src/persist.cpp†L120-L220】 |

## 13. Additional Resources

- `analysis/report.md` — consolidated build/benchmark evidence and Done-When checklist.【F:analysis/report.md†L1-L120】
- `cpu_surrogate_trainer_implementation_specs.md` — historical specification for each project phase.
- `tests/` directory — practical usage references for every subsystem (tensors, kernels, layers, losses, optimizers, trainer, persistence, logging, dataloaders).【F:tests/test_tensor.cpp†L1-L160】【F:tests/test_mathkernels.cpp†L1-L320】【F:tests/test_optimizer.cpp†L1-L180】

With the above instructions, you can configure and extend the CPU Surrogate Trainer on any supported platform while preserving the deterministic validation pipeline.
